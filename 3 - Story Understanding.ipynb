{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the final submission for SNLP at UCL 2017/18. It was completed in groups of 3 with scores for both code and understanding. It achieved a final mark of 90%. It focused on Deep Learning for NLP story understanding, with the use of GloVe and LSTMs amongst other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last assignment, you will apply deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding [[1]](http://anthology.aclweb.org/D/D13/D13-1020.pdf). Specifically, you will develop a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story [[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf). This sounds (and to an extent is) trivial for humans, however it is quite a difficult task for machines as it involves commonsense knowledge and temporal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "You are given a dataset of 45502 instances, each consisting of 5 sentences. Your system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In this particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2017/assignment3/problem/group_X/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to, and in `X` in `group_X` contains the number of your group.\n",
    "\n",
    "After you placed it there, **rename the notebook file** to `group_X.ipynb`.\n",
    "\n",
    "The notebook is pre-set to save models in\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2017/assignment3/problem/group_X/model/\n",
    "\n",
    "Be sure not to tinker with that directory - we expect your submission to contain a `model` subdirectory with a single saved model! \n",
    "The saving procedure might overwrite the latest save, or not. Make sure you understand what it does, and upload only a single model! (for more details check tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit, move nor copy these cells**.\n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit, move, nor copy these cells**.\n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "**If you edit, move or copy any of the setup, assessments and mark cells, you will be penalised with -20 points**.\n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment nor the dataset publicly, by uploading it online, emailing it to friends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. **Do not edit the next cell, nor copy/duplicate it**. Instead refer to the variables in your own code, and slice and dice them as you see fit (but do not change their values). \n",
    "For example, no one stops you from introducing, in the corresponding task section, `my_train` and `my_dev` variables that split the data into different folds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Notice that the data is loaded from tab-separated files. The files are easy to read, and we provide the loading functions that load it into a simple data structure. Feel free to check details of the loading.\n",
    "\n",
    "The data structure at hand is an array of dictionaries, each containing a `story` and the `order` entry. `story` is a list of strings, and `order` is a list of integer indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.134033",
     "start_time": "2016-12-20T12:04:57.115270"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order': [3, 2, 1, 0, 4],\n",
       " 'story': ['His parents understood and decided to make a change.',\n",
       "  'The doctors told his parents it was unhealthy.',\n",
       "  'Dan was overweight as well.',\n",
       "  \"Dan's parents were overweight.\",\n",
       "  'They got themselves and Dan on a diet.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation\n",
    "\n",
    "Your primary task in this assignment is to implement a model that produces the right order of the sentences in the dataset.\n",
    "\n",
    "### Preprocessing pipeline\n",
    "\n",
    "First, we construct a preprocessing pipeline, in our case `pipeline` function which takes care of:\n",
    "- out-of-vocabulary words\n",
    "- building a vocabulary (on the train set), and applying the same unaltered vocabulary on other sets (dev and test)\n",
    "- making sure that the length of input is the same for the train and dev/test sets (for fixed-sized models)\n",
    "\n",
    "You are free (and encouraged!) to do your own input processing function. Should you experiment with recurrent neural networks, you will find that you will need to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.842961",
     "start_time": "2016-12-20T12:04:57.136946"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# convert train set to integer IDs\n",
    "train_stories, train_orders, vocab = nn.pipeline(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that the `pipeline` function returns the necessary data for your computational graph feed - the required inputs in this case, as we will call this function to process your dev and test data. If you do not make sure that the same pipeline applied to the train set is applied to other datasets, your model may not work with that data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.925263",
     "start_time": "2016-12-20T12:04:59.844598"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# get the length of the longest sentence\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_orders, _ = nn.pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take a look at the result of the `pipeline` with the `show_data_instance` function to make sure that your data loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.954655",
     "start_time": "2016-12-20T12:04:59.926701"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " Story:\n",
      "  The manager decided to offer John the job.\n",
      "  During the interview he was very <OOV> and <OOV>\n",
      "  He went to the interview very prepared and nicely dressed.\n",
      "  John was excited to have a job interview.\n",
      "  The manager of the company was really impressed by John's comments.\n",
      " Order:\n",
      "  [4 2 1 0 3]\n",
      "\n",
      "Desired story:\n",
      "  John was excited to have a job interview.\n",
      "  He went to the interview very prepared and nicely dressed.\n",
      "  During the interview he was very <OOV> and <OOV>\n",
      "  The manager of the company was really impressed by John's comments.\n",
      "  The manager decided to offer John the job.\n"
     ]
    }
   ],
   "source": [
    "nn.show_data_instance(dev_stories, dev_orders, vocab, 155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertGloveTF(vocab, vocab_key):\n",
    "    print(\"Begin\")\n",
    "\n",
    "    pretrainedVectors = loadGlovePretrainedVectors('glove.6B.300d.txt')\n",
    "\n",
    "    vector_dim = 200\n",
    "\n",
    "    # Randomly initialise OOV vec\n",
    "    oov_vec = np.random.rand(1, vector_dim)\n",
    "\n",
    "    embeddings = np.zeros((len(vocab), vector_dim))\n",
    "\n",
    "    # deal with <OOV> and <PAD>\n",
    "    embeddings[0, :] = np.ones([1, vector_dim])  # <PAD> - set as ones\n",
    "    embeddings[1, :] = oov_vec  # <OOV> \n",
    "\n",
    "    for i in range(2, len(vocab_key)):\n",
    "        word = vocab_key[i]\n",
    "        try:\n",
    "            embeddings[i, :] = pretrainedVectors[word]\n",
    "        except:\n",
    "            embeddings[i, :] = oov_vector\n",
    "\n",
    "    np.save('embeddings', embeddings)\n",
    "\n",
    "\n",
    "def loadGlovePretrainedVectors(file):\n",
    "    print(\"Loading Glove pre-trained word vectors\")\n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    vectors = {}\n",
    "    for line in f:\n",
    "        lineSegment = line.split()\n",
    "        word = lineSegment[0]\n",
    "        embedding = []\n",
    "        for vector in lineSegment[1:]:\n",
    "            embedding.append(float(vector))\n",
    "        vectors[word] = embedding\n",
    "\n",
    "    print(\"Finished loading\")\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# token = re.compile(\"[\\w-]+|'t|'ll|’re|’n|'ve|'d|'m|'s|\\'\")\n",
    "# def tokenize(input):\n",
    "#     return [word.lower() for word in token.findall(input)]\n",
    "\n",
    "# def pipeline(data, vocab=None, max_sent_len_=None):\n",
    "#     is_ext_vocab = True\n",
    "#     if vocab is None:\n",
    "#         is_ext_vocab = False\n",
    "#         vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "#     max_sent_len = -1\n",
    "#     data_sentences = []\n",
    "#     data_orders = []\n",
    "#     for instance in data:\n",
    "#         sents = []\n",
    "#         for sentence in instance['story']:\n",
    "#             sent = []\n",
    "#             tokenized = tokenize(sentence)\n",
    "#             for token in tokenized:\n",
    "#                 if not is_ext_vocab and token not in vocab:\n",
    "#                     vocab[token] = len(vocab)\n",
    "#                 if token not in vocab:\n",
    "#                     token_id = vocab['<OOV>']\n",
    "#                 else:\n",
    "#                     token_id = vocab[token]\n",
    "#                 sent.append(token_id)\n",
    "#             if len(sent) > max_sent_len:\n",
    "#                 max_sent_len = len(sent)\n",
    "#             sents.append(sent)\n",
    "#         data_sentences.append(sents)\n",
    "#         data_orders.append(instance['order'])\n",
    "\n",
    "#     if max_sent_len_ is not None:\n",
    "#         max_sent_len = max_sent_len_\n",
    "#     out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "#     for i, elem in enumerate(data_sentences):\n",
    "#         for j, sent in enumerate(elem):\n",
    "#             out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "#     out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "#     return out_sentences, out_orders, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model we provide is a rudimentary, non-optimised model that essentially represents every word in a sentence with a fixed vector, sums these vectors up (per sentence) and puts a softmax at the end which aims to guess the order of sentences independently.\n",
    "\n",
    "First we define the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train + data_dev\n",
    "train_stories, train_orders, vocab = nn.pipeline(data_train)\n",
    "max_sent_len = train_stories.shape[2]\n",
    "\n",
    "# convert dev set to integer IDs, based on the train vocabulary and max_sent_len\n",
    "dev_stories, dev_orders, _ = nn.pipeline(data_dev, vocab=vocab, max_sent_len_=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:59.966529",
     "start_time": "2016-12-20T12:04:59.956638"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Configuration\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS = 5 #you can change it to 5, not defined in the training yet (if you want you can assign it, now is 5)\n",
    "\n",
    "NUM_LAYERS = 3 ## You can put 1,2 or 3\n",
    "NUM_UNITS = 200 ## You can put more\n",
    "KEEP_PROB = { 0: 0.5, 1: 0.5, 2: 0.5, 3: 0.3, 4: 0.3, 5: 0.3}\n",
    "TARGET_SIZE = 5\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "input_size = 100 #EMBEDDING, You can use 20\n",
    "output_size = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### MODEL ###\n",
    "\n",
    "#max_sent_len is the maximum sentence length\n",
    "\n",
    "#We are going to concatenate the 5 stories\n",
    "total_sequence = 5 * max_sent_len   \n",
    "\n",
    "## Placeholders\n",
    "story = tf.placeholder(tf.int64, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "order = tf.placeholder(tf.int64, [None, None], \"order\")              # [batch_size x 5]\n",
    "\n",
    "batch_size = tf.shape(story)[0]\n",
    "keep_prob = tf.placeholder(tf.float32) #Value of probability to keep during dropout\n",
    "\n",
    "sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(axis = 1, num_or_size_splits=5, value = story)]  # 5 times [batch_size x max_length]\n",
    "\n",
    "# Word embeddings\n",
    "\n",
    "embeddingArray = np.load('./embeddings100.npy') ### commented for inference\n",
    "embeddings_row, embeddings_col = embeddingArray.shape\n",
    "initializer = tf.constant_initializer(embeddingArray)\n",
    "\n",
    "\n",
    "embeddings = tf.get_variable(\"x\", [vocab_size, input_size], initializer=initializer, dtype=tf.float64)\n",
    "\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 X[batch_size x max_seq_length x input_size]\n",
    "                      for sentence in sentences]\n",
    "\n",
    "# Model architecture\n",
    "\n",
    "#Concatenate all sentences\n",
    "inputs = tf.cast(tf.concat(sentences_embedded,axis = 1), tf.float32) # size [batch_size X (5 * max_)]\n",
    "# [batch_size x (5 * max_seq_length) x embedding_size]\n",
    "\n",
    "def build_cell(lstm_size, keep_prob):\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(lstm_size, state_is_tuple=True)\n",
    "        \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop\n",
    "\n",
    "#Lstm cell\n",
    "#lstm = tf.nn.rnn_cell.LSTMCell(NUM_UNITS, state_is_tuple=True) #I changed this in order to be compatible\n",
    "\n",
    "#dropout \n",
    "#dropout = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob = keep_prob) #I changed this in order to be compatible\n",
    "\n",
    "#stack of cells\n",
    "stack = tf.nn.rnn_cell.MultiRNNCell([build_cell(NUM_UNITS, keep_prob) for _ in range(NUM_LAYERS)], state_is_tuple=True)\n",
    "#stack = tf.nn.rnn_cell.MultiRNNCell([dropout for _ in range(NUM_LAYERS)], state_is_tuple=True)\n",
    "\n",
    "#outputs, after the sequence of RNN\n",
    "outputs, state = tf.nn.dynamic_rnn(stack, inputs, dtype=tf.float32)\n",
    "\n",
    "#reshape output shape[total_sequence * NUM_UNITS, batch_size]\n",
    "output_h = tf.reshape(outputs, [-1, total_sequence * NUM_UNITS])\n",
    "\n",
    "#weights of last layer, shape[total_sequence * num_units, 5 * target_size]\n",
    "W = tf.get_variable(\"w\", [total_sequence * NUM_UNITS, 5 * TARGET_SIZE], dtype=tf.float32)\n",
    "\n",
    "#biases of last layer, shape[5 * target_size]\n",
    "b = tf.get_variable(\"b\", [5 * TARGET_SIZE], dtype=tf.float32)\n",
    "\n",
    "#get logits after linear combination shape [batch_size, (5 * target_size) ]\n",
    "logits_flat = tf.add(tf.matmul(output_h, W),b) \n",
    "\n",
    "#reshape to the correct size\n",
    "logits = tf.reshape(logits_flat, [-1, 5, TARGET_SIZE])        # [batch_size x 5 x target_size]x\n",
    "\n",
    "# loss \n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=order))\n",
    "\n",
    "l2 = 0.005 * sum(tf.nn.l2_loss(loss)for tf_var in tf.trainable_variables()\n",
    "                 if not (\"noreg\" in tf_var.name or \"b\" in tf_var.name))\n",
    "loss += (l2)\n",
    "\n",
    "# prediction function\n",
    "unpacked_logits = [tensor for tensor in tf.unstack(logits, axis=1)]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "softmaxed_logits = tf.stack(softmaxes, axis=1)\n",
    "predict = tf.arg_max(softmaxed_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built our model, together with the loss and the prediction function, all we are left with now is to build an optimiser on the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:01.184409",
     "start_time": "2016-12-20T12:05:00.997016"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0005, beta1=0.9, beta2=0.99, epsilon=1e-8)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "opt_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n",
    "We defined the preprocessing pipeline, set the model up, so we can finally train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.615600",
     "start_time": "2016-12-20T12:05:01.186008"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        n = train_stories.shape[0]\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('----- Epoch', epoch, '-----')\n",
    "            indices = np.random.random_sample(((n // BATCH_SIZE), ))\n",
    "            indices = np.argsort(indices)\n",
    "            total_loss = 0\n",
    "            for i in range(n // BATCH_SIZE):\n",
    "                inst_story = train_stories[indices[i] * BATCH_SIZE: (indices[i] + 1) * BATCH_SIZE]\n",
    "                inst_order = train_orders[indices[i] * BATCH_SIZE: (indices[i] + 1) * BATCH_SIZE]\n",
    "                feed_dict = {story: inst_story, order: inst_order, keep_prob: KEEP_PROB[epoch]}\n",
    "                _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "                total_loss += current_loss\n",
    "\n",
    "            print(' Train loss:', total_loss / n)\n",
    "\n",
    "            train_feed_dict = {story: train_stories, order: train_orders, keep_prob: 1.0}\n",
    "            train_predicted = sess.run(predict, feed_dict=train_feed_dict)\n",
    "            train_accuracy = nn.calculate_accuracy(train_orders, train_predicted)\n",
    "            print(' Train accuracy:', train_accuracy)\n",
    "\n",
    "            dev_feed_dict = {story: dev_stories, order: dev_orders, keep_prob:1.0}\n",
    "            dev_predicted = sess.run(predict, feed_dict=dev_feed_dict)\n",
    "            dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "            print(' Dev accuracy:', dev_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        nn.save_model(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Assess Accuracy (40 pts) \n",
    "\n",
    "We assess how well your model performs on an unseen test set. We will look at the accuracy of the predicted sentence order, on sentence level, and will score them as followis:\n",
    "\n",
    "* 0 - 10 pts: 45% <= accuracy < 50%, linear\n",
    "* 10 - 20 pts: 50% <= accuracy < 55, linear\n",
    "* 20 - 40 pts: 55 <= accuracy < 60, linear\n",
    "* extra 0-10 pts: 60 <= accuracy < 70, linear\n",
    "\n",
    "The **linear** mapping maps any accuracy value between the lower and upper bound linearly to a score. For example, if your model's accuracy score is $acc=54.5\\%$, then your score is $10 + 10\\frac{acc-50}{55-50}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following lines so that they construct the test set in the same way you constructed the dev set in the code above. We will insert the test set instead of the dev set here. **`test_feed_dict` variable must stay named the same**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "# make sure you process this with the same pipeline as you processed your dev set\n",
    "test_stories, test_orders, _ = nn.pipeline(data_test, vocab=vocab, max_sent_len_=max_sent_len)\n",
    "\n",
    "# THIS VARIABLE MUST BE NAMED `test_feed_dict`\n",
    "test_feed_dict = {story: test_stories, order: test_orders, keep_prob: 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64489577765900585"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 1 is marked with ** __ points**. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "Enter a 1000 words max description of your approach **in this cell**.\n",
    "Make sure to provide:\n",
    "- an **error analysis** of the types of errors your system makes\n",
    "- compare your system with the model we provide, focus on differences and draw useful comparations between them\n",
    "\n",
    "Should you need to include figures in your report, make sure they are Python-generated (matplotlib, seaborn, bokeh are all included in the stat-nlp-book Docker image). For that, feel free to create new cells after this cell (before Assessment 2 cell). Link online images at your risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivations\n",
    "\n",
    "The aim of this assignment is to construct deep learning models that would determine the correct sequence of a story. The first successful application of deep learning models in sequence-to-sequence learning tasks was by ([Sutskevar et al. 2014](https://arxiv.org/pdf/1409.3215.pdf)). This paper applied a multi-layered Long Short-Term Memory (LSTM) to map the input sequence to a vector of fixed dimensionality and then another deep LSTM to decode the target sequence from the vector. Since then, LSTMs and its variants have become the de-facto approach to sequence learning and mapping problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "**1) Preprocessing**\n",
    "\n",
    "**1.1) Word Embedding**\n",
    "\n",
    "The baseline model presented in our stub represents words as discrete ids. This leads to a problem of data sparsity when trying to contextualize our words as part of a sentence. To overcome this we vectorise our words using GloVe embedding. Vectorizing our words alleviates the data sparsity problem as similar words are recognised as similar or the same vector.\n",
    "\n",
    "Given the size restraint of our assignment file we applied the GloVe model (pretrained with the Wikipedia and Gigaword datasets) with 100 dimensions. We do not use all the word embeddings when training, only the embeddings for the words in our vocabulary. We deal with OOVs by setting such words to the UNK token and assigning them same vector to them. We experimented with 200 and 300 dimensions but due to memory constraints we deemed further experimentation infeasible. For the epochs that we did train successfully, no significant difference was noted. Implementing GloVe boosted the score on our LSTM by 0.5 - 1%.\n",
    "\n",
    "We analysed how many tokens in our vocab were not among the pre-trained word embeddings and fortunately it was a small amount. In the event a large proportion of our vocab was not among the pre-trained word embeddings, we would have attempted to train our own word embeddings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Model**\n",
    "\n",
    "![model](https://scontent-lht6-1.xx.fbcdn.net/v/t1.0-9/26219123_10155922352942969_8715478189856124103_n.jpg?oh=d8b7eee8f9e8909ed13b61a6f2f41714&oe=5AFD8190)\n",
    "\n",
    "The baseline model is very simplistic, its approach is to take the input sentence and convert it into a single value which is dependent on the length of a sentence. So the baseline model learns how to order the sentences by length. We implement an LSTM model which very much focuses on the sequence of words in the sentence. This is the critical difference between our model and the original baseline. Capturing sequential relationships helps our model perform better.\n",
    "\n",
    "Our final model consists of stacked layers LSTM cells. The inputs into the model are the word embeddings, these were originally initialised but later replaced with the output of the GloVE preprocessing. In order to capture the dependencies between the sentences in the story, we concatenated the five sentences per input; where each sentence was already replaced by the embedding. After each LSTM cell we used dropout to stop the model from overfitting, before feeding the outputs into the next cell in the stack. This is done via the MultiRNNCell and dynamic_rnn cells. Most of LSTM models use between 100 and 200 units. We realized that using 200 units performed slightly better than using 100. Besides, using more LSTM layers resulted in more overfitting, that is why we decided to stick with only 3 layers. We use the sparse matrix cross entropy loss to calculate the loss along with L2 regularization to help evaluate the model before using the Adam Optimiser. \n",
    "\n",
    "With regards to similarities between our model and the original. Like the original, we use word embeddings and so move to a numerical representation of our data. From our experience, vectorising helps the model learn better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) Training**\n",
    "\n",
    "As well as implementing a new model, we also improve upon the default implementation by implementing better training procedures. \n",
    "Gradient Clipping\n",
    "Since our model has 3 LSTM layers with 200 units cells working in tandem over many time steps, they could have derivatives that are either very large or very small. Gradient clipping is a technique that helps prevent exploding gradients in such networks. Through gradient clipping we normalize the gradients of parameter vector when its value exceeds a certain threshold. ([*Bengio, 2013*](https://arxiv.org/abs/1211.5063))\n",
    "\n",
    "\\begin{equation*}\n",
    "New Gradient = Old Gradient*\\frac{threshold}{L2-norm(gradients)}\n",
    "\\end{equation*}\n",
    "\n",
    "**Random Batching**\n",
    "\n",
    "To ensure our mini-batches are not highly correlated with one another we are shuffling our data using random batching. This also helps prevent oscillation of weight values that occur from one epoch to the next, and to make weight space more stochastic.  ([*Goodfellow, 2016*](http://www.deeplearningbook.org/)). In practical terms, we shuffle the indices of the data every epoch before drawing batches. The addition of random batching does result in a marginal improvement in our accuracy of 0.5%.\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "Adding extra layers to our deep architecture increases the probability of overfitting our training data. One way to prevent this is by implementing dropout. This ensures that our neurons activate with only a certain probability. We implemented a dictionary of dropout probabilities in order to slow down the overfitting as the model ran each epoch. \n",
    "L2-Regularization To compliment our stochastic regularizer (dropout), we also added non-stochastic L2-regularization. This adds a penalty on the square value of our LSTM weights and ensures they only function if there are big gradients to counteract them. This increased both the consistency and accuracy of our final model. Bringing us to a final accuracy of 55.5% on the dev set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimisation**\n",
    "\n",
    "In order to tune our final model we used gridsearch to find the optimal values needed for Adam, the number of layers and the word embedding dimensions. We found that 3 layers worked best and allowed us to keep the model small enough to stay inside the constraints. Adam seemed to overfit with higher than 0.0005 learning rate, we also decayed the learning rate as the model trained in order to avoid overfitting. These were all optimised using cross-validation, although the initial grid searches had to contain few parameters as there was a time constraint due to work with LSTMs.\n",
    "\n",
    "**Checkpoints**\n",
    "\n",
    "Our final model achieved its’ maximum accuracy on epoch 4/20, therefore early-stopping was used in order to allow us to save the model so that it can be used for inference. There was also a patience period added to the model in order to stop the training if the model loss stopped decreasing, the final training was stopped at epoch 12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Failed Approaches**\n",
    "\n",
    "There were a number of additional techniques we applied that were not in the baseline code, however these did not improve our score. Regex Tokenization, we used regular expressions to split up the sentences with the hope that it would perform better than splitting up by the spaces. We tried several regular expressions before settling on one that would work well with apostrophes. This increased our validation accuracy by 1. However, we were not able to get GloVe working in conjunction with custom tokenization and therefore decided to omit it.\n",
    "\n",
    "We switched from a uniform initializer to truncated normal before switching over to GloVe. Truncated normal proved better than uniform and achieved scores of 53.5% on the dev set with basic testing as opposed to 52.5% from uniform. It also appeared to converge faster. \n",
    "\n",
    "Some investigation was done into activation functions. The final model used Tanh, however we tested with relu and sigmoid, neither improved our accuracy and gradient clipping was more effective at reducing the gradients per epoch. \n",
    "\n",
    "GRU Cells were tried, these provided little change from the LSTM cells we used in the final model and often achieved a lower score while being more consistent between epochs. Therefore these were not used in the final model.\n",
    "\n",
    "## Comparison with Baseline Model\n",
    "\n",
    "| Model | Accuracy |\n",
    "| :---         |         ---: |\n",
    "| LSTM (Final)     | 55.6%   |\n",
    "| Baseline     | 40%     |\n",
    "\n",
    "## Other Models\n",
    "\n",
    "| Model |  Accuracy |\n",
    "| :---         |        ---: |\n",
    "| LSTM + GloVe + L2 | 55.3%   |\n",
    "| LSTM + GloVe     | 54%     |\n",
    "| LSTM + regex token     | 53%     |\n",
    "| basic LSTM/GRU   | 51%     |\n",
    "| MLP     | 32%     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error analysis**\n",
    "\n",
    "Our model was able to predict the first (72%) and final sentences (63%) most accurately, this was due to the nature of those sentences. When using english, we build upon the sentences in such a way that the first and last sentences are usually the shortest, often containing just a main clause. When taking the mean of the 5 positions in the sentences across the dataset we found this to be the case.\n",
    "\n",
    "Another reason for the first sentence being easy to predict is that the first sentence will include information that will be control by a personal pronoun later in the story. These are short stories and thus names are usually given in the first sentence. For example, when looking at the following stories we can see where the model can correctly predict the first sentence but also where it struggles.\n",
    "\n",
    "Story 1: “Frank had been drinking beer. He got a call from his girlfriend, asking where he was. Frank suddenly realized he had a date that night. Since Frank was already a bit drunk, he could not drive. Frank spent the rest of the night drinking more beers.” \n",
    "\n",
    "This follows 1 of the 2 rules defined in the first paragraph, the first sentence is short, however the model was unable to correctly label the rest of the sentences due to the use of “Frank” and the similarity of these sentences. \n",
    "Story 2: “Josh had a parrot that talked. During show and tell, Josh's parrot said a bad word. He brought his parrot to school. The teacher told Joshua not to bring his bird again. When Josh got home, he was grounded.” \n",
    "This story has a simpler format than the previous, it was guessed correctly by our final model. Again it follows the same rules underlined in the first paragraph of the error analysis. Josh is the object of the story and this is easy to see.\n",
    "\n",
    "Other than these rules the english sentence structure allows for compound and complex sentences. These are sentences which contain a main clause but also other clauses. These are usually connected by connectives like “and” or “because”. These are usually seen in sentence 2, 3 or 4. This is apparent in Story 2.\n",
    "All of these contribute to the length of the sentences in a numeric way, this is very easy for the model to pick up and thus increases our accuracy the more apparent these rules are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess Description (60 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did, or you did nothing)\n",
    "* Creativity (25pts: we could not have come up with this, 0pts: Use only the provided model)\n",
    "* Substance (25pts: implemented complex state-of-the-art classifier, compared it to a simpler model, 0pts: Only use what is already there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 2 is marked with ** __ points**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Final mark</font>: Your solution to Assignment 3 is marked with ** __points**. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
